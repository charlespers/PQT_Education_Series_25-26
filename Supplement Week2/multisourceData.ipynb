{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ddecb7",
   "metadata": {},
   "source": [
    "This project explores a realistic approach to modelling market information from different sources.\n",
    "\n",
    "The provided dataset contains information on: \n",
    "\n",
    "- Equity prices and volumes\n",
    "\n",
    "- Option Greeks and implied volatility\n",
    "\n",
    "- Realized volatility\n",
    "\n",
    "- Sentiment data\n",
    "\n",
    "- Market indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99439374",
   "metadata": {},
   "source": [
    "Our goals in this project are to explore feature engineering, cross model correlations, and predictive modeling\n",
    "\n",
    "Along the way we will also learn about key financial components such as volatility and sentiment and relavent considerations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d90ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/data.csv\", parse_dates=[\"date\",\"expiration_date\"])\n",
    "cols = df.columns\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaea2f5",
   "metadata": {},
   "source": [
    "We have a lot of features to consider here.\n",
    "\n",
    "One useful approach to feature engineering is to group our features based on their domains.\n",
    "\n",
    "We see that in our data we can naturally group our data by:\n",
    "\n",
    "- stock\n",
    "\n",
    "- option\n",
    "\n",
    "- greeks\n",
    "\n",
    "- volatility\n",
    "\n",
    "- sentiment\n",
    "\n",
    "- earnings\n",
    "\n",
    "- vix\n",
    "\n",
    "Grouping by economic meaning helps prevent thoughtless correlation based dropping later For example delta, gamma, and vega will all appear highly correlated in plots, but each reflects a different sensitivity dimension of an otions contract. Dropping one without context can hinder interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32b04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups = {\n",
    "    \"stock\": ['stock_open','stock_high','stock_low','stock_close','stock_traded_volume'],\n",
    "    \"option\": ['options_close_price','options_volume','strike','open_interest'],\n",
    "    \"greeks\": ['delta','gamma','theta','vega','rho','epsilon','lambda','d1','d2'],\n",
    "    \"volatility\": ['implied_vol','realized_vol','realized_vol_diff_target','7_day_realized_vol_target','7_day_implied_vol_target','implied_vol_diff_target'],\n",
    "    \"vix\": ['vix-open','vix-high','vix-low','vix-close'],\n",
    "    \"sentiment\": ['article_sentiment','pos_total_count','neu_total_count','neg_total_count','total_count'],\n",
    "    \"earnings\": ['reported_estimate_eps_percent_diff','pos_em_count','neg_em_count','em_total_count'],\n",
    "}\n",
    "for group, cols in feature_groups.items():\n",
    "    print(f\"\\n{group} features: {len(cols)}\")\n",
    "    display(df[cols].describe().T.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05bc18f",
   "metadata": {},
   "source": [
    "Correlation helps visualize redundancy: if two features are ~1 correlated one might be redundant. \n",
    "\n",
    "Remember we want the our unit feature space's hypercube to be as small as possible so that our feature space is occupied by our datapoints vs. mostly empty in higher dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27439c6f",
   "metadata": {},
   "source": [
    "Spearman correlation detects monotonic relationships, not just linear ones\n",
    "\n",
    "This matters in cases where variables move in a non linear pattern (Think greeks due to their black-scholes derivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_report(df, cols):\n",
    "    pairs = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i+1, len(cols)):\n",
    "            rho, p = spearmanr(df[cols[i]], df[cols[j]], nan_policy='omit')\n",
    "            if abs(rho) > 0.7:\n",
    "                pairs.append((cols[i], cols[j], rho))\n",
    "    return pd.DataFrame(pairs, columns=['Feature_1', 'Feature_2', 'Spearman_rho'])\n",
    "\n",
    "for group, cols in feature_groups.items():\n",
    "    print(f\"\\n Highly correlated pairs in {group.upper()}:\")\n",
    "    display(correlation_report(df, cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d1245",
   "metadata": {},
   "source": [
    "Almost immediatly we see that some features like stockopen, stockclose, stockhigh, are ~1 correlated, so adding them all to our model unnessisarily inflates our feature space.\n",
    "\n",
    "Lets also grasp a visual understanding of these dependencies using heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7ffae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group, cols in feature_groups.items():\n",
    "    corr = df[cols].corr()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b27993",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Here we perform some commmon transforms and explain the cocepts behind them:\n",
    "\n",
    "- Price and volume data is often non-stationary, meaning their mean and variance change over time. Machine learning assumes stationary data. We can fix this with logs, returns, and ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logs\n",
    "df['log_stock_close'] = np.log1p(df['stock_close'])\n",
    "df['log_option_close'] = np.log1p(df['options_close_price'])\n",
    "df['log_traded_volume'] = np.log1p(df['stock_traded_volume'])\n",
    "\n",
    "# Returns\n",
    "df['stock_return'] = df['stock_close'].pct_change()\n",
    "df['option_return'] = df['options_close_price'].pct_change()\n",
    "\n",
    "# Ratios\n",
    "df['option_stock_ratio'] = df['options_close_price'] / df['stock_close']\n",
    "df['implied_realized_vol_ratio'] = df['implied_vol'] / df['realized_vol']\n",
    "df['bid_ask_spread'] = (df['ask'] - df['bid']) / df['stock_close']\n",
    "\n",
    "# Rolling avgs\n",
    "df['rolling_stock_vol_7d'] = df['stock_return'].rolling(7).std()\n",
    "df['rolling_option_vol_7d'] = df['option_return'].rolling(7).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509a6e09",
   "metadata": {},
   "source": [
    "\n",
    "- lagged: lagged returns avoid data leakage by ensuring we only use infromation available at time t-1 to predict time t. For example if one of our columns was \"Next_day_returns\" this would allow our model to peak into information for time t+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc85350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lags \n",
    "for lag in [1, 3, 5, 7]:\n",
    "    df[f'lag_stock_return_{lag}'] = df['stock_return'].shift(lag)\n",
    "    df[f'lag_implied_vol_{lag}'] = df['implied_vol'].shift(lag)\n",
    "    df[f'lag_option_return_{lag}'] = df['option_return'].shift(lag)\n",
    "    \n",
    "# --- Calendar features ---\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['days_to_expiration'] = (df['expiration_date'] - df['date']).dt.days\n",
    "\n",
    "# --- Cyclic encoding for day of week ---\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531d6757",
   "metadata": {},
   "source": [
    "- Options greeks and voltilities are connected by nonlinear surfaces. If you've studied options or taken a financial instruments class you might be familiar with volatility smiles/skews which option greeks tend to follow\n",
    "\n",
    "- When we know data follows a certain distribution we should do our best to model that curvature. \n",
    "\n",
    "A brief explanation of the changes below is provided, but dont worry about following with the finance jargon too much:\n",
    "\n",
    "- vol skew models asymmetry of risk expectations for out of money vs at money vs in money options\n",
    "- vol surf curvature models gamma vega structure showing sensitivity of iv to the underlying\n",
    "- volvol models the volatility of volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70967943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vol curves\n",
    "df['vol_skew'] = df['d1'] - df['d2']\n",
    "df['vol_surface_curvature'] = df['gamma'] / (df['vega'] + 1e-6)\n",
    "\n",
    "# Vol change \n",
    "df['iv_change'] = df['implied_vol'].pct_change()\n",
    "df['rv_change'] = df['realized_vol'].pct_change()\n",
    "df['vol_premium_change'] = df['implied_realized_vol_ratio'].pct_change()\n",
    "\n",
    "# volvol\n",
    "df['vol_vol'] = df['iv_change'] * df['rv_change']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d2ba8",
   "metadata": {},
   "source": [
    "- Sentiment data gives you 'orthoginal information' or information not directly from price or volume but from traders \n",
    "\n",
    "Sentiment ratios normalize counts and weight scores market tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aea7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize by article count\n",
    "for sentiment_type in ['pos_total_count', 'neu_total_count', 'neg_total_count']:\n",
    "    df[f'{sentiment_type}_ratio'] = df[sentiment_type] / (df['total_count'] + 1e-6)\n",
    "\n",
    "# weifht sentiment score\n",
    "df['weighted_sentiment'] = (\n",
    "    df['article_sentiment'] * \n",
    "    (df['pos_total_count_ratio'] - df['neg_total_count_ratio'])\n",
    ")\n",
    "\n",
    "# rolling mean for smoothness\n",
    "df['sentiment_rolling_mean_7d'] = df['weighted_sentiment'].rolling(7).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64292ffe",
   "metadata": {},
   "source": [
    "Finally we consider interaction across domains\n",
    "\n",
    "- does volatility rise on down days? (leveraging effect)\n",
    "\n",
    "- macro micro perceptions (VIX to IV)\n",
    "\n",
    "- are greeks related in explanatory ways "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b9504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stock vol relations\n",
    "df['return_x_iv'] = df['stock_return'] * df['implied_vol']\n",
    "df['return_x_vix'] = df['stock_return'] * df['vix-close']\n",
    "df['iv_x_sentiment'] = df['implied_vol'] * df['weighted_sentiment']\n",
    "\n",
    "# greeks\n",
    "df['vega_x_theta'] = df['vega'] * df['theta']\n",
    "df['delta_x_gamma'] = df['delta'] * df['gamma']\n",
    "\n",
    "# micro macro\n",
    "df['vix_iv_ratio'] = df['vix-close'] / (df['implied_vol'] + 1e-6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a223b",
   "metadata": {},
   "source": [
    "Many of the new features are heavy tailed or have probabilities concentrated at extremes (ends of dist)\n",
    "\n",
    "To prevent these from dominating model learning we can use robust normalization which for learning purposes is less sensitive to outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ff5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_features = [\n",
    "    'stock_return','option_return','implied_realized_vol_ratio','bid_ask_spread',\n",
    "    'vol_skew','vol_surface_curvature','iv_change','rv_change','vol_vol',\n",
    "    'weighted_sentiment','sentiment_rolling_mean_7d','return_x_iv','vega_x_theta','vix_iv_ratio'\n",
    "]\n",
    "\n",
    "scaler = RobustScaler()\n",
    "df[engineered_features] = scaler.fit_transform(df[engineered_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6be686",
   "metadata": {},
   "source": [
    "Lets check our new features making sure they are centered, have meaningful relations, and avoid redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df[engineered_features].corr(), cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Heatmap of Engineered Features\")\n",
    "plt.show()\n",
    "\n",
    "df[engineered_features].hist(bins=40, figsize=(14, 10))\n",
    "plt.suptitle(\"Distribution of Engineered Features\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d62d7",
   "metadata": {},
   "source": [
    "If you thought this was a lot of work thats because most of time building models is spent in the data processing/feature selection phases. These aren't always intuitive relationships and often require teams and researchers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba043e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
